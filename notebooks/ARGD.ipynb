{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive Diffusion Models on Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "from torch_geometric.datasets import ZINC\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.nn import MessagePassing, GAT\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "from benchmarks.GraphARM.models import DiffusionOrderingNetwork, DenoisingNetwork\n",
    "from benchmarks.GraphARM.utils import NodeMasking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciate the dataset\n",
    "dataset = ZINC(root='../data/ZINC', transform=None, pre_transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[18, 1], edge_index=[2, 36], edge_attr=[36], y=[1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point = dataset[1]\n",
    "point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
       "        2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point.edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# absence of edges should be an edge type, attribute 0\n",
    "# TODO fully connect graph with unexisting edges in the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4907])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [4],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [2],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0]]),\n",
       " 'edge_index': tensor([[ 0,  1,  1,  2,  2,  3,  3,  4,  4,  4,  5,  6,  6,  6,  7,  8,  8,  8,\n",
       "           9, 10, 10, 11, 11, 11, 12, 12, 13, 13, 14, 14, 14, 15, 16, 16, 17, 17],\n",
       "         [ 1,  0,  2,  1,  3,  2,  4,  3,  5,  6,  4,  4,  7,  8,  6,  6,  9, 10,\n",
       "           8,  8, 11, 10, 12, 17, 11, 13, 12, 14, 13, 15, 16, 14, 14, 17, 11, 16]]),\n",
       " 'edge_attr': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
       "         2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2]),\n",
       " 'y': tensor([0.4907])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node decay ordering, for forward absorbing pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](media/don.png)\n",
    "\n",
    "![Alt text](media/don_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0546, 0.0551, 0.0559, 0.0574, 0.0578, 0.0589, 0.0567, 0.0560, 0.0557,\n",
      "        0.0557, 0.0552, 0.0545, 0.0544, 0.0544, 0.0544, 0.0545, 0.0544, 0.0544],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caio/mambaforge/envs/imitation/lib/python3.8/site-packages/torch_geometric/utils/scatter.py:98: UserWarning: scatter_reduce() is in beta and the API may change at any time. (Triggered internally at  ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:1550.)\n",
      "  return src.new_zeros(size).scatter_reduce_(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_ord_net = DiffusionOrderingNetwork(node_feature_dim=1,\n",
    "                                        num_node_types=dataset.x.unique().shape[0] + 1,\n",
    "                                        num_edge_types=3,\n",
    "                                        num_layers=3,\n",
    "                                        out_channels=1)\n",
    "sigma_t_dist = diff_ord_net(point)\n",
    "print(sigma_t_dist.flatten())\n",
    "# sample from categorical distribution to get node to mask\n",
    "# TODO only on the samples that are not masked\n",
    "# sigma_t = F.softmax(sigma_t_dist.flatten(), dim=0)\n",
    "torch.distributions.Categorical(probs=sigma_t_dist.flatten()).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = NodeMasking(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Diffusion Process\n",
    "\n",
    "Node ordering $\\sigma$ using diffusion ordering network. Exactly one node decays at a time.\n",
    "\n",
    "At each step $t$, distribution of $t$-th node is conditioned on original graph $G$ and generated node ordering $\\sigma$ up to $t-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_decay_ordering(datapoint):\n",
    "    p = datapoint.clone()\n",
    "    node_order = []\n",
    "    for i in range(p.x.shape[0]):\n",
    "        # use diffusion ordering network to get probabilities\n",
    "        sigma_t_dist = diff_ord_net(p, i)\n",
    "        # sample from categorical distribution to get node to mask\n",
    "        # TODO only on the samples that are not masked\n",
    "        unmasked = ~masker.is_masked(p)\n",
    "        sigma_t = torch.distributions.Categorical(probs=sigma_t_dist[unmasked].flatten()).sample()\n",
    "        \n",
    "        # get node index\n",
    "        sigma_t = torch.where(unmasked.flatten())[0][sigma_t.long()]\n",
    "        node_order.append(sigma_t)\n",
    "        # mask node\n",
    "        p = masker.mask_node(p, sigma_t)\n",
    "    return node_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(12),\n",
       " tensor(6),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(7),\n",
       " tensor(11),\n",
       " tensor(1),\n",
       " tensor(13),\n",
       " tensor(3),\n",
       " tensor(15),\n",
       " tensor(8),\n",
       " tensor(10),\n",
       " tensor(2),\n",
       " tensor(9),\n",
       " tensor(16),\n",
       " tensor(14),\n",
       " tensor(17)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_decay_ordering(point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Diffusion (Generative) Process\n",
    "\n",
    "Denoising network $p_ \\theta (G_t | G_{t+1})$ is a graph attention network (GAT). (Vanilla GAT)\n",
    "\n",
    "For now, simple graph convolutional network (GCN) is used.\n",
    "\n",
    "\n",
    "** Initially, the graph is fully connected and masked, so in the paper they only keep the masked node to be denoised during the generation step. \n",
    "\n",
    "For now, we will just use the whole graph as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMessage passing\\nTODO find out if custom message passing function is necessary\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Message passing\n",
    "TODO find out if custom message passing function is necessary\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure attentive message passing is done correctly.\n",
    "\n",
    "\n",
    "![Alt text](media/image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.nn import Linear, Parameter\n",
    "class MessagePassingLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MessagePassingLayer, self).__init__(aggr='mean', flow=\"target_to_source\")  # 'add' aggregation for simplicity\n",
    "\n",
    "        self.W = nn.Linear(in_channels, out_channels)\n",
    "        # self.attention = nn.Linear(2 * in_channels, attention_dim)\n",
    "        self.bias = Parameter(torch.empty(out_channels))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.W.reset_parameters()\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        # Step 4: Normalize node features.\n",
    "        return norm.view(-1, 1) * x_j # TODO change to adapt attention mechanism\n",
    "    \n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        # Attention mechanism\n",
    "        # alpha = F.softmax(self.attention(torch.cat([x[row], x[col]], dim=-1)), dim=-1)\n",
    "        # alpha = F.dropout(alpha, p=0.5)\n",
    "        # messages = alpha.view(-1, 1) * edge_attr\n",
    "        # messages = scatter_add(messages, col, dim=0, dim_size=x.size(0))\n",
    "\n",
    "        # h = self.W(torch.cat([x, messages], dim=-1))\n",
    "\n",
    "        out = self.propagate(edge_index, x=x, norm=norm)\n",
    "        out += self.bias\n",
    "\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](media/architecture.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 29 3 7 1\n"
     ]
    }
   ],
   "source": [
    "# num_features\n",
    "denoising_net = DenoisingNetwork(\n",
    "    node_feature_dim=dataset.num_features,\n",
    "    num_node_types=dataset.x.unique().shape[0] + 1,\n",
    "    num_edge_types=3,\n",
    "    num_layers=7,\n",
    "    out_channels=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = point.clone()\n",
    "node_type_probs, edge_type_probs = denoising_net(p)\n",
    "edge_type_probs # connections of new node to all previous nodes\n",
    "node_type_probs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "For now, use MSE / Reconstruction error instead of loss function from the paper.\n",
    "\n",
    "\n",
    "Optimize by sampling multiple ($M$) diffusion trajectories, thereby enabling training both the diffusion ordering network $q_ϕ(σ|G_0)$ and the denoising network\n",
    "$p_θ(Gt|G_t+1)$ using gradient descent.\n",
    "\n",
    "Create M trajectories (sequence of graphs) for each training graph. Where node decay order is sampled from $q_ϕ(σ|G_0)$ (or random, initially).\n",
    "\n",
    "Train denoising network to minimize the negative VLB using SGD.\n",
    "\n",
    "Diffusion ordering network can be updated with common RL optimization methods, e.g., the REINFORCE algorithm. Creating M trajectories and computing the negative VLB to obtain rewards, and then updating the parameters of the diffusion ordering network using the REINFORCE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 29 3 7 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcaiofreitas\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/caio/workspace/GraphDiffusionImitate/notebooks/wandb/run-20231101_163341-yxz3kiul</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/caiofreitas/ARGD/runs/yxz3kiul' target=\"_blank\">denoising_and_ordering</a></strong> to <a href='https://wandb.ai/caiofreitas/ARGD' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/caiofreitas/ARGD' target=\"_blank\">https://wandb.ai/caiofreitas/ARGD</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/caiofreitas/ARGD/runs/yxz3kiul' target=\"_blank\">https://wandb.ai/caiofreitas/ARGD/runs/yxz3kiul</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/caiofreitas/ARGD/runs/yxz3kiul?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f3bc99ade80>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use wandb to log the model\n",
    "import wandb\n",
    "\n",
    "denoising_net = DenoisingNetwork(\n",
    "    node_feature_dim=dataset.num_features,\n",
    "    num_node_types=dataset.x.unique().shape[0] + 1,\n",
    "    num_edge_types=3,\n",
    "    num_layers=7,\n",
    "    out_channels=1\n",
    ")\n",
    "\n",
    "wandb.init(\n",
    "        project=\"ARGD\",\n",
    "        group=f\"v0.5\",\n",
    "        name=f\"denoising_and_ordering\",\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"policy\": \"train\",\n",
    "            \"n_epochs\": 10000,\n",
    "            \"batch_size\": 1,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](media/optimizer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.20it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.96it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.56it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.68it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.38it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.40it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.64it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.82it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.46it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.51it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.49it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.68it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.61it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.94it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.55it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.30it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.31it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.57it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.49it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# train denoising diffusion model\n",
    "# use node decay ordering\n",
    "\n",
    "EPSILON = 1e-8\n",
    "\n",
    "optimizer = torch.optim.Adam(denoising_net.parameters(), lr=1e-4, betas=(0.9, 0.999))\n",
    "ordering_optimizer = torch.optim.Adam(diff_ord_net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "\n",
    "M = 4 # number of diffusion trajectories to be created for each graph\n",
    "\n",
    "for superbatch in range(10, 20):\n",
    "    loss = 0\n",
    "    with tqdm(range(4*superbatch, 2+4*superbatch)) as pbar:\n",
    "        denoising_net.train()\n",
    "        diff_ord_net.eval()\n",
    "        for batch in pbar:\n",
    "            graph = dataset[batch]\n",
    "            # print(f\"Generating trajectories for graph\")\n",
    "            # node decay ordering, accoding to node_decay_ordering\n",
    "            original_data = graph.clone()\n",
    "            diffusion_trajectories = []\n",
    "            for m in range(M):\n",
    "                node_order = node_decay_ordering(graph)\n",
    "                \n",
    "                # create diffusion trajectory\n",
    "                diffusion_trajectory = [original_data]\n",
    "                masked_data = graph.clone()\n",
    "                for node in node_order:\n",
    "                    masked_data = masked_data.clone()\n",
    "                    \n",
    "                    masked_data = masker.mask_node(masked_data, node)\n",
    "                    diffusion_trajectory.append(masked_data)\n",
    "\n",
    "                diffusion_trajectories.append(diffusion_trajectory)\n",
    "            \n",
    "            # predictions & loss\n",
    "            for diffusion_trajectory in diffusion_trajectories:\n",
    "                G_0 = diffusion_trajectory[0]\n",
    "                # optimizer.zero_grad()\n",
    "                node_order = node_decay_ordering(G_0)\n",
    "                for t in range(1, len(diffusion_trajectory)-1):\n",
    "                    node = node_order[len(diffusion_trajectory)-t-1]\n",
    "                    G_t = diffusion_trajectory[t]\n",
    "                    # transform to float\n",
    "                    G_t.x = G_t.x.float()\n",
    "                    G_t.edge_index = G_t.edge_index.float()\n",
    "\n",
    "                    G_tplus1 = diffusion_trajectory[t+1].clone()\n",
    "                    # transform to float\n",
    "                    G_tplus1.x = G_tplus1.x.float()\n",
    "                    G_tplus1.edge_index = G_tplus1.edge_index.float()\n",
    "\n",
    "                    G_pred = G_tplus1.clone()\n",
    "                    \n",
    "\n",
    "                    # predict node type\n",
    "                    node_type_probs, edge_type_probs = denoising_net(G_pred)\n",
    "                    \n",
    "                    #  Apply multinomial distribution\n",
    "                    # node_dist = torch.distributions.Multinomial(probs=node_type_probs.squeeze(), total_count=1)\n",
    "                    # node_type = node_dist.sample()\n",
    "                    # edge_dist = torch.distributions.Multinomial(probs=edge_type_probs.squeeze(), total_count=G_tplus1.x.shape[0])\n",
    "                    # node_connections = edge_dist.sample()\n",
    "\n",
    "                    # add node type to node\n",
    "                    # node_dist = torch.distributions.Categorical(probs=node_type_probs.squeeze())\n",
    "                    # node_type = node_dist.sample()\n",
    "                    # print(node_type)\n",
    "                    # G_pred.x[node] = node_type\n",
    "                    # sample edge type\n",
    "                    # new_connections = torch.multinomial(node_connections_probs.squeeze(), num_samples=1, replacement=True)\n",
    "                    '''\n",
    "                    TODO make sure you \n",
    "                    \"predict the connections of the new node to all previous nodes at once \n",
    "                                            using a mixture of multinomial distribution\"\n",
    "                    '''\n",
    "                    # add new connections to edge_attr\n",
    "                    # for i in  range(len(node_connections)):\n",
    "                    #     new_connection = node_connections[i]\n",
    "                    #     if new_connection != 0:\n",
    "                    #         G_pred.edge_attr[G_pred.edge_index[0] == i] = new_connection\n",
    "                    \n",
    "                    # calculate loss\n",
    "                    p_O_v =  node_type_probs[node].mean() + EPSILON # TODO add edges (joint probability)\n",
    "                    w_k = diff_ord_net(G_tplus1)[node]\n",
    "                    n_i = G_t.x.shape[0]\n",
    "                    loss -= (n_i/(len(diffusion_trajectory)-1))*torch.log(p_O_v)*w_k\n",
    "    \n",
    "    loss /= M\n",
    "    # backprop\n",
    "    loss.backward()\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # log loss\n",
    "    pbar.set_description(f\"Epoch: {batch}, Loss: {loss.item()%10:.4f}\")\n",
    "    wandb.log({\"loss\": loss.item()})\n",
    "    # validation batch (for diffusion ordering network)\n",
    "\n",
    "    reward = 0\n",
    "    with tqdm(range(2+4*superb\n",
    "        self.diffusion_ordering_network.train()atch, 4+4*superbatch)) as pbar:\n",
    "        for i in pbar:\n",
    "            graph = dataset[i]\n",
    "            n_i = graph.x.shape[0]\n",
    "            original_data = graph.clone()\n",
    "            diffusion_trajectories = []\n",
    "            \n",
    "            denoising_net.eval()\n",
    "            diff_ord_net.train()\n",
    "\n",
    "            # Generate M diffusion trajectories\n",
    "            for m in range(M):\n",
    "                node_order = node_decay_ordering(original_data)\n",
    "                \n",
    "                # create diffusion trajectory\n",
    "                diffusion_trajectory = [original_data]\n",
    "                masked_data = graph.clone()\n",
    "                for node in node_order:\n",
    "                    masked_data = masked_data.clone()\n",
    "                    \n",
    "                    masked_data = masker.mask_node(masked_data, node)\n",
    "                    diffusion_trajectory.append(masked_data)\n",
    "\n",
    "                diffusion_trajectories.append(diffusion_trajectory)\n",
    "\n",
    "            for diffusion_trajectory in diffusion_trajectories:\n",
    "                G_0 = diffusion_trajectory[0]\n",
    "                node_order = node_decay_ordering(G_0)\n",
    "                for t in range(len(diffusion_trajectory)-1):\n",
    "                    node = node_order[G_0.x.shape[0] - t - 1]\n",
    "                    G_tplus1 = diffusion_trajectory[t+1]\n",
    "                    # predict node type\n",
    "                    node_type_probs, edge_type_probs = denoising_net(G_tplus1)\n",
    "                    # node_type_probs.register_hook(lambda grad: print(grad.mean()))\n",
    "\n",
    "                    # node_type_probs = node_type_probs[node]\n",
    "                    #  Apply multinomial distribution\n",
    "                    # node_dist = torch.distributions.Multinomial(probs=node_type_probs.squeeze(), total_count=1)\n",
    "                    # node_type = node_dist.sample()\n",
    "\n",
    "                    # calculate reward (VLB)\n",
    "                    \n",
    "                    p_O_v =  node_type_probs[node].mean() + EPSILON # TODO add edges (joint probability)\n",
    "\n",
    "                    # reward -= torch.log(O_v)*n_i/(len(diffusion_trajectory)-1)\n",
    "                    r = (n_i/(len(diffusion_trajectory)-1))*torch.log(p_O_v)\n",
    "                    w_k = diff_ord_net(G_tplus1)[node]\n",
    "\n",
    "                    reward -= w_k*r\n",
    "    reward /= M\n",
    "    wandb.log({\"reward\": reward.item()})\n",
    "    # update parameters (REINFORCE algorithm)\n",
    "    ordering_optimizer.zero_grad()\n",
    "    reward.backward()\n",
    "    ordering_optimizer.step()\n",
    "    pbar.set_description(f\"Epoch: {batch}, Loss: {reward.item()%10:.4f}\")\n",
    "    # save model\n",
    "    torch.save(denoising_net.state_dict(), \"ardm_model_overfit.pt\")\n",
    "    torch.save(diff_ord_net.state_dict(), \"ordering_model_overfit.pt\")\n",
    "                \n",
    "    '''\n",
    "    TODO: edge_attributes have to be used. The masked nodes can be identified through them. There'll never be error in edge_index\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_pred = diffusion_trajectory[-1].clone()\n",
    "G_pred.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13)\n",
      "tensor(27)\n",
      "tensor(19)\n",
      "tensor(26)\n",
      "tensor(24)\n",
      "tensor(25)\n",
      "tensor(5)\n",
      "tensor(11)\n",
      "tensor(1)\n",
      "tensor(26)\n",
      "tensor(20)\n",
      "tensor(4)\n",
      "tensor(26)\n",
      "tensor(6)\n",
      "tensor(25)\n",
      "tensor(19)\n",
      "tensor(2)\n",
      "tensor(24)\n",
      "tensor(24)\n",
      "tensor(2)\n",
      "tensor(19)\n",
      "tensor(19)\n",
      "tensor(15)\n",
      "tensor(28)\n",
      "tensor(16)\n",
      "tensor(11)\n",
      "tensor(24)\n",
      "tensor(13)\n",
      "tensor(15)\n",
      "tensor(24)\n",
      "tensor(25)\n",
      "tensor(22)\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "# compute node_order\n",
    "node_order = node_decay_ordering(G_0)\n",
    "\n",
    "forward_pass = []\n",
    "\n",
    "for i in node_order:\n",
    "    forward_pass.append(G_pred.clone())\n",
    "    # 1 diffusion step for each node\n",
    "    with torch.no_grad():\n",
    "        node_type_probs, edge_type_probs = denoising_net(G_pred)\n",
    "        # sample node type\n",
    "        node_dist = torch.distributions.Categorical(probs=node_type_probs.squeeze())\n",
    "        node_type = node_dist.sample()[i]\n",
    "        print(node_type)\n",
    "        edge_dist = torch.distributions.Multinomial(probs=edge_type_probs.squeeze(), total_count=G_tplus1.x.shape[0])\n",
    "        node_connections = edge_dist.sample()\n",
    "        G_pred.x[i] = node_type\n",
    "        # add new connections to edge_attr\n",
    "        # for i in  range(len(node_connections)):\n",
    "        #     new_connection = node_connections[i]\n",
    "        #     if new_connection != 0:\n",
    "        #         G_pred.edge_attr[G_pred.edge_index[0] == i] = new_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28],\n",
       "        [28]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_pass[0].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    denoising_net(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imitation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
